# -*- coding: utf-8 -*-
"""278a_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10FDxEMkoEqzlDQnufXqAp4f4BK8yGJW6
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from keras.layers import MultiHeadAttention, LayerNormalization, Layer, Dense, Dropout, Flatten, Embedding, Add
import cv2
import skimage.measure
import matplotlib.image as mpimg
from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/MyDrive/278a/'
from google.colab.patches import cv2_imshow

"""### encoder model"""

N = 40
D = 512
Dff = 2048
H = 8
L = 6
dropout_rate = 0.1
vocab_length = 2048
output_dim = 10

batch_size = 16
epochs = 1

def positional_encoding(length, depth):
  depth = depth/2

  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)
  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)

  angle_rates = 1 / (10000**depths)         # (1, depth)
  angle_rads = positions * angle_rates      # (pos, depth)

  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)

  return tf.cast(pos_encoding, dtype=tf.float32)

class PositionalEmbedding(Layer):
  def __init__(self, vocab_size=vocab_length, dim_model=D):
    super().__init__()
    self.d_model = dim_model
    self.embedding = Embedding(input_dim=vocab_size, output_dim=dim_model, mask_zero=False)
    self.pos_encoding = positional_encoding(length=2048, depth=dim_model)

  def call(self, x):
    length = tf.shape(x)[1]
    x = self.embedding(x)
    # This factor sets the relative scale of the embedding and positonal_encoding.
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x = x + self.pos_encoding[tf.newaxis, :length, :]
    return x

class BaseAttention(Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = MultiHeadAttention(**kwargs)
    self.layernorm = LayerNormalization()
    self.add = Add()

class GlobalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(query=x, value=x, key=x)
    x = self.add([x, attn_output])
    return self.layernorm(x)

class FeedForward(Layer):
  def __init__(self, dim_model=D, dim_mlp=Dff, dropout_rate=dropout_rate):
    super().__init__()
    self.seq = tf.keras.Sequential([
      Dense(dim_mlp, activation='relu'),
      Dense(dim_model),
      Dropout(dropout_rate)
    ])
    self.add = Add()
    self.layer_norm = LayerNormalization()

  def call(self, x):
    x = self.add([x, self.seq(x)])
    return self.layer_norm(x)

class Encoder(Layer):
  def __init__(self,*, dim_model=D, num_heads=H, dim_mlp=Dff, dropout_rate=dropout_rate):
    super().__init__()
    self.self_attention = GlobalSelfAttention(
        num_heads=num_heads,
        key_dim=dim_model,
        dropout=dropout_rate)
    self.mlp = FeedForward(dim_model=dim_model, dim_mlp=dim_mlp)

  def call(self, x):
    x = self.self_attention(x)
    x = self.mlp(x)
    return x

class GestureIdentifier(tf.keras.Model):
    def __init__(self, *, output_dim=output_dim, dim_model=D, dim_mlp=Dff, L=L, dropout_rate=dropout_rate):
        super().__init__()
        self.dim_model = dim_model
        self.L = L

        self.embed = PositionalEmbedding()
        self.encoder_layers = [Encoder() for _ in range(L)]
        self.dropout = Dropout(dropout_rate)

        self.mlp_head = tf.keras.Sequential([
          Dense(dim_mlp, activation='relu'),
          Dense(output_dim)
        ])

    def call(self, x): # x is (None, N) where None refers to batch size
        x = self.embed(x) # (None, N, dim_model)
        x = self.dropout(x)
        for i in range(self.L): x = self.encoder_layers[i](x)

        flatten = Flatten()
        x = flatten(x) # (None, N*dim_model)
        y = tf.nn.softmax(self.mlp_head(x)) # (None, output_dim)

        return y

"""### pre-training setup"""

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, dim_model=D, warmup_steps=5000):
    super().__init__()

    self.dim_model = dim_model
    self.dim_model = tf.cast(self.dim_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    step = tf.cast(step, dtype=tf.float32)
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.dim_model) * tf.math.minimum(arg1, arg2)

# create optimizer
learning_rate = CustomSchedule(dim_model=D)
optimizer = tf.keras.optimizers.legacy.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

# make batches
train_seqs = tf.convert_to_tensor(np.load(path+'dataset/training_sequences.npy'), dtype=tf.int32)
train_labels = tf.convert_to_tensor(np.load(path+'dataset/training_labels.npy'), dtype=tf.int32)
test_inputs = tf.convert_to_tensor(np.load(path+'dataset/testing_sequences.npy'), dtype=tf.int32)
test_outputs = tf.convert_to_tensor(np.load(path+'dataset/testing_labels.npy'), dtype=tf.int32)
test_inputs_ordered = np.load(path+'dataset/testing_sequences_ordered.npy')
test_outputs_ordered = np.load(path+'dataset/testing_labels_ordered.npy')

# set aside 10% of the training samples for validation
train_len = int(len(train_labels)*0.9/batch_size)*batch_size
val_len = len(train_labels) - train_len

train_inputs = train_seqs[:train_len]
val_inputs = train_seqs[train_len:train_len+val_len]
train_outputs = train_labels[:train_len]
val_outputs = train_labels[train_len:train_len+val_len]

# define loss and accuracy functions
def loss(truths, preds):
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
  loss = loss_object(truths, preds)
  return loss

def gesture_accuracy(truths, preds):
  preds = tf.math.argmax(preds,1)
  truths = tf.cast(truths,tf.int64)
  match = truths == preds
  match = tf.cast(match, dtype=tf.int32)
  return tf.reduce_sum(match)/tf.size(match)

"""### show training examples"""

def seq2image(seq,Q=10):
  img = np.zeros(400,int)
  b = [pow(2,i) for i in range(Q)]
  for i in range(N):
    btoken = format(seq[i], 'b')
    btoken = np.append([0]*(Q-len(btoken)),[btoken[j] for j in range(len(btoken))])
    btoken = btoken[::-1]
    img[i*Q:i*Q+Q] = btoken
  img = img.reshape(20,20)
  return img

# show random train images
fig, axs = plt.subplot_mosaic([[np.random.randint(len(train_outputs)) for i in range(3)] for j in range(3)], layout='constrained')
for label, ax in axs.items():
    ax.set_title('Gesture '+str(int(train_outputs[label])))
    ax.imshow(seq2image(train_inputs[label]))
    ax.axis('off')
plt.show()

# show test images of a specific gesture
gesture = 9
fig, axs = plt.subplot_mosaic([[300*gesture+i+j*3 for i in range(3)] for j in range(3)], layout='constrained')
for label, ax in axs.items():
    ax.set_title('Gesture '+str(int(test_outputs_ordered[label])))
    ax.imshow(seq2image(test_inputs_ordered[label]))
    ax.axis('off')
plt.show()

"""### training

initial training
"""

model = GestureIdentifier()
model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])
model.fit(train_inputs, train_outputs, batch_size = batch_size, epochs = epochs, validation_data = (val_inputs, val_outputs))

# save the weights
model.save_weights(path+'gesture_identifier_full')

"""### testing"""

temp_model = GestureIdentifier()
temp_model.load_weights(path+'gesture_identifier_full')

"""#### test accuracy on test set"""

gesture_indeces = [300*i for i in range(output_dim+1)]
gesture_accuracies = np.zeros(output_dim)
for i in range(len(gesture_indeces)-1):
  preds = temp_model(tf.convert_to_tensor(test_inputs_ordered[gesture_indeces[i]:gesture_indeces[i+1]], dtype=tf.int32))
  truths = test_outputs_ordered[gesture_indeces[i]:gesture_indeces[i+1]]
  gesture_accuracies[i] = 100*gesture_accuracy(truths,preds).numpy()

np.save(path+'gesture_accuracies.npy',gesture_accuracies)

acc = np.load(path+'gesture_accuracies.npy')
plt.bar(range(output_dim),acc)
plt.title('Accuracy on Each Gesture')
plt.xlabel('Gesture')
plt.xticks(range(output_dim))
plt.ylabel('Accuracy (%)')
plt.show()
print()
print(str(np.mean(acc)) + ' % average accuracy')

"""#### test accuracy on images from google"""

def binarize(image,threshold=135):
  N = np.size(image,1)
  return np.array([[1 if image[i][j] > threshold else 0 for j in range(N)] for i in range(N)],int)

def image2seq(image,Q=10):
  b = [pow(2,i) for i in range(Q)]
  image_flat = image.flatten()
  image_flat = np.append(image_flat,np.zeros(np.mod(len(image_flat),Q),int))
  seq = np.zeros(int(len(image_flat)/Q),int)
  for i in range(len(seq)):
      seq[i] = image_flat[i*Q:i*Q+Q] @ b
  return seq

# fetch sample asl images
imgs = []
for i in range(9):
  imgs.append(cv2.imread(path+'samples/'+str(i+1)+'.png'))
imgs = np.array(imgs)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
    ax.set_title('Gesture '+str(label+1))
    ax.imshow(imgs[label])
    ax.axis('off')
plt.show()

# grayscale
imgs_g = np.array([img[:,:,0] for img in imgs])
imgs_g = np.array([binarize(skimage.measure.block_reduce(img, (14,14), np.mean),40) for img in imgs_g],np.uint8)

imgs_g = imgs_g[:9,:50,:50]

# make the fingers skinnier
trim = 2 # pixels
kernel = np.ones((trim,trim),np.uint8)
imgs_t = np.zeros(imgs_g.shape,np.uint8)
for i in range(len(imgs_g)):
  imgs_t[i] = cv2.erode(imgs_g[i],kernel)

# pool
imgs_p = np.array([cv2.resize(img,(20,20)) for img in imgs_t])

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_p)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
    ax.set_title('Gesture '+str(label+1))
    ax.imshow(imgs_p[label])
    ax.axis('off')
plt.show()

def soft_plot(soft):
  plt.imshow(soft.numpy())
  plt.title('Softmax Outputs')
  plt.xlabel('Gesture')
  plt.ylabel('Image')
  if len(soft) > 10:
    plt.yticks(np.arange(len(soft)),[str(i+1) if np.mod(i,5) == 0 else ' ' for i in range(len(soft))])
  else:
    plt.yticks(np.arange(len(soft)),[str(i+1) for i in range(len(soft))])
  plt.xticks(np.arange(output_dim))
  plt.colorbar()
  plt.show()

seqs = [image2seq(imgs_p[i]) for i in range(len(imgs_p))]
input = tf.convert_to_tensor(seqs, dtype=tf.int32)
preds = temp_model(input)
soft_plot(preds)

"""#### test accuracy on non-blended images"""

imgs = []
for i in range(9):
  imgs.append((cv2.imread(path+'samples/'+str(i+1)+'_b.jpg')))

for i in range(len(imgs)):
  if i <= 5: imgs[i] = imgs[i][:,::-1]

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
    ax.set_title('Gesture '+str(label+1))
    ax.imshow(imgs[label])
    ax.axis('off')
plt.show()

# downsample
imgs_g = [cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),(20,20)) for img in imgs]
imgs_g = np.array([binarize(img,100) for img in imgs_g],np.uint8)


fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_g)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
    ax.set_title('Gesture '+str(label+1))
    ax.imshow(imgs_g[label])
    ax.axis('off')
plt.show()

seqs = [image2seq(imgs_g[i]) for i in range(len(imgs_g))]
input = tf.convert_to_tensor(seqs, dtype=tf.int32)
preds = temp_model(input)
soft_plot(preds)

"""#### test accuracy on images from the simple thresholding segmentation method"""

imgs_simple = []
for i in range(9):
  imgs_simple.append(cv2.imread(path+'brian_segmentation/segmented/Gesture'+str(i+1)+'_simple_thresh.jpg'))
imgs_simple = np.array(imgs_simple)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_simple)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Simple ' + str(label+1))
  ax.imshow(imgs_simple[label])
  ax.axis('off')
plt.show()

# convert to grayscale
imgs_simple = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[:,::-1] for img in imgs_simple],float)

# downsample
imgs_simple = np.array([binarize(cv2.resize(img,(20,20)),200) for img in imgs_simple],float)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_simple)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Simple ' + str(label+1))
  ax.imshow(imgs_simple[label])
  ax.axis('off')
plt.show()

seqs = [image2seq(imgs_simple[i]) for i in range(len(imgs_simple))]
input = tf.convert_to_tensor(seqs, dtype=tf.int32)
preds = temp_model(input)
soft_plot(preds)

"""#### test accuracy on images from the otsu thresholding segmentation method"""

imgs_otsu = []
for i in range(9):
  imgs_otsu.append(cv2.imread(path+'brian_segmentation/segmented/Gesture'+str(i+1)+'_otsu_thresh.jpg'))
imgs_otsu = np.array(imgs_otsu)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_otsu)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Otsu ' + str(label+1))
  ax.imshow(imgs_otsu[label])
  ax.axis('off')
plt.show()

# convert to grayscale
imgs_otsu = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[:,::-1] for img in imgs_otsu],float)

# downsample
imgs_otsu = np.array([binarize(cv2.resize(img,(20,20)),200) for img in imgs_otsu],float)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_otsu)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Otsu ' + str(label+1))
  ax.imshow(imgs_otsu[label])
  ax.axis('off')
plt.show()

seqs = [image2seq(imgs_otsu[i]) for i in range(len(imgs_otsu))]
input = tf.convert_to_tensor(seqs, dtype=tf.int32)
preds = temp_model(input)
soft_plot(preds)

"""#### test accuracy on images from the otsu contouring segmentation method"""

imgs_otsu = []
for i in range(9):
  imgs_otsu.append(cv2.imread(path+'brian_segmentation/segmented/Gesture'+str(i+1)+'_otsu_contour.jpg'))
imgs_otsu = np.array(imgs_otsu)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_otsu)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Otsu Contour' + str(label+1))
  ax.imshow(imgs_otsu[label])
  ax.axis('off')
plt.show()

# convert to grayscale
imgs_otsu = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in imgs_otsu],float)
img_otsu = np.array([img[:,::-1] if i < 6 else img for img in imgs_otsu],float)

# downsample
imgs_otsu = np.array([binarize(cv2.resize(img,(20,20)),200) for img in imgs_otsu],float)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_otsu)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Otsu Contour' + str(label+1))
  ax.imshow(imgs_otsu[label])
  ax.axis('off')
plt.show()

seqs = [image2seq(imgs_otsu[i]) for i in range(len(imgs_otsu))]
input = tf.convert_to_tensor(seqs, dtype=tf.int32)
preds = temp_model(input)
soft_plot(preds)

"""#### test accuracy on images from the adaptive contouring segmentation method"""

imgs_adaptive = []
for i in range(9):
  imgs_adaptive.append(cv2.imread(path+'brian_segmentation/segmented/Gesture'+str(i+1)+'_adaptive_contour.jpg'))
imgs_adaptive = np.array(imgs_adaptive)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_adaptive)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Adaptive ' + str(label+1))
  ax.imshow(imgs_adaptive[label])
  ax.axis('off')
plt.show()

# convert to grayscale
imgs_adaptive = np.array([cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in imgs_adaptive],float)
imgs_adaptive = np.array([img[:,::-1] if i < 6 else img for img in imgs_adaptive],float)

# downsample
imgs_adaptive = np.array([binarize(cv2.resize(img,(20,20)),200) for img in imgs_adaptive],float)

fig, axs = plt.subplot_mosaic([[j*3+i for i in range(int(len(imgs_adaptive)/3))] for j in range(3)], layout='constrained')
for label, ax in axs.items():
  ax.set_title('Adaptive ' + str(label+1))
  ax.imshow(imgs_adaptive[label])
  ax.axis('off')
plt.show()

seqs = [image2seq(imgs_adaptive[i]) for i in range(len(imgs_adaptive))]
input = tf.convert_to_tensor(seqs, dtype=tf.int32)
preds = temp_model(input)
soft_plot(preds)